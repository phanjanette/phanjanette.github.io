---
title: "assignment05"
format: html
---

```{r}
## Scraping Government data
## Website: GovInfo (https://www.govinfo.gov/app/search/)
## Prerequisite: Download from website the list of files to be downloaded
## Designed for background job

# Start with a clean plate and lean loading to save memory
 
gc(reset=T)

# install.packages(c("purrr", "magrittr")
library(purrr)
library(magrittr) # Alternatively, load tidyverse

## Set path for reading the listing and home directory
## For Windows, use "c:\\directory\\subdirectory\\"
## For Mac, "/Users/YOURNAME/path/"

setwd("/Users/janettephan/documents/GitHub/phanjanette.github.io/assignment05")
library(rjson)
library(jsonlite)
library(data.table)
library(readr)

## CSV method
govfiles= read.csv(file="https://github.com/datageneration/datamethods/raw/refs/heads/master/webdata/govinfo-search-results-2024-10-13T07_10_42.csv", skip=2)

## JSON method
### rjson
gf_list <- rjson::fromJSON(file ="https://github.com/datageneration/datamethods/raw/refs/heads/master/webdata/govinfo-search-results-2024-10-13T07_18_29.json")
govfile2=dplyr::bind_rows(gf_list$resultSet)

### jsonlite
gf_list1 = jsonlite::read_json("https://github.com/datageneration/datamethods/raw/refs/heads/master/webdata/govinfo-search-results-2024-10-13T07_18_29.json")

### Extract the list
govfiles3 <- gf_list1$resultSet

### One more step
govfiles3 <- gf_list1$resultSet |> dplyr::bind_rows()

### Sorting publishDate newest to oldest
govfiles3 <- govfiles3 |>
  dplyr::mutate(publishdate = as.Date(publishdate)) |>
  dplyr::arrange(dplyr::desc(publishdate))

# Preparing for bulk download of government documents
govfiles$id = govfiles$packageId
pdf_govfiles_url = govfiles3$pdfLink
pdf_govfiles_id <- govfiles3$index

# Directory to save the pdf's
# Be sure to create a folder for storing the pdf's
save_dir <- "/Users/janettephan/documents/GitHub/phanjanette.github.io/assignment05"

# Function to download pdfs
download_govfiles_pdf <- function(url, id) {
  tryCatch({
    destfile <- paste0(save_dir, "govfiles_", id, ".pdf")
    download.file(url, destfile = destfile, mode = "wb") # Binary files
    Sys.sleep(runif(1, 1, 3))  # Important: random sleep between 1 and 3 seconds to avoid suspicion of "hacking" the server
    return(paste("Successfully downloaded:", url))
  },
  error = function(e) {
    return(paste("Failed to download:", url))
  })
}

### Download 10 most recent documents
start.time <- Sys.time()
message("Starting downloads")
results <- 1:10 %>% 
  purrr::map_chr(~ download_govfiles_pdf(pdf_govfiles_url[.], pdf_govfiles_id[.]))
message("Finished downloads")
end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken
print(results)
```
## Difficulties Encountered:
I had trouble figuring out the correct date variable because it wasnâ€™t clearly documented in the dataset. Setting up the working directory also took time since I needed to find the right folder where my downloaded files would go and be easy to locate. Another challenge was simply understanding the code itself, as it took a while to learn what each part was doing and how the pieces worked together.

## Usefulness of the Scraped Data:
The scraped data is usable and includes the main information I expected, such as the document title, text, and publication date. After skimming the downloaded documents, they appear accurate, though they would still need closer review to confirm nothing important is missing.

## How to Improve?:
The scraping process could be improved by using an API instead of downloading files manually. An API would provide cleaner data and make the process faster and more reliable.

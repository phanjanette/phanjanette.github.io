[
  {
    "objectID": "reviews.html",
    "href": "reviews.html",
    "title": "Reviews",
    "section": "",
    "text": "Podcast Review: The Next Industrial Revolution is Industrial AI\nTo get ready for class 2, I listened to a recent Dataframed podcast episode called “The Next Industrial Revolution is Industrial AI” with Barbara Humpton (CEO, Siemens USA) and Olympia Brikis (Director of Industrial AI at Siemens USA).\nThe general theme was industrial AI versus consumer AI. Consumer AI is found in ordinary life, like a recommendation feature or a chatbot on a website. It doesn’t matter when it makes mistakes because there is little risk and is not as strict. Industrial AI, on the other hand, runs inside factories, power generation, and other mechanical processes. It has to be extremely safe and trustworthy because little mistakes can destroy a factory line or pose safety risks.\nWhat is most interesting is that industrial AI is being used not to replace humans but to help them. The speakers explained how technologies like predictive maintenance, computer-vision quality checks, and digital twins (virtual copies of a manufacturing line) allow factories to work better. For example, a digital twin can simulate a new line before it’s ever built, cutting setup time from years to months. A different story illustrated how computer vision systems of today inspect each car door in real-time as opposed to spot checks, which provides assurance of quality at a much greater scale. The show also brought out the “human + machine” partnership. Instead of laying off individuals, companies have the same workers but use them more efficiently. Workers depart from monotonous manual tasks, like inspecting every part manually, to exceptions and process optimization. I liked this because it showed a better future for work, especially in light of the specter that AI is going to take people’s jobs away. AI as a means to upskill, not fear.\nAlthough, I would have liked to see more numbers and metrics. They mentioned timelines and substantial productivity gains, but it would have been nice to see specific metrics. I also would have liked to hear more about concerns like safety risks, data quality, or model learning in extreme factory environments.\nOverall, it is a great primer for those who want to know how AI is already changing manufacturing. It is especially helpful for plant managers, engineers, and policymakers thinking about workforce development. For me, and especially for one who is open to adopting AI as new technology, the most important takeaway is that industrial AI is not about substituting humans, but it’s about humans and AI working together to make industries smarter, faster, and safer."
  },
  {
    "objectID": "assignment01.html",
    "href": "assignment01.html",
    "title": "Assignment 1",
    "section": "",
    "text": "This website is created using RStudio Quarto with the Cosmo theme for a clean and professional look. The navigation bar includes multiple tabs that connect to different areas of content, such as the Home page, Assignments for EPPS 6302: Methods of Data Collection and Production, and my CV/Resume.\nWhile it is currently designed to organize and showcase course assignments, the site will gradually develop into a portfolio of projects that reflects my academic and professional interests."
  },
  {
    "objectID": "assignment02.html",
    "href": "assignment02.html",
    "title": "Assignment 2: Google Trends Data",
    "section": "",
    "text": "I downloaded the CSV from Google Trends for the terms “Trump, Kamala, Election” (past 12 months).\nThe CSV provides weekly intervals of search interest for each term.\n\n# Load manual CSV\nmanual_df &lt;- read.csv(\"assignment02/TrumpHarrisElection.csv\", skip = 2)\n\n# Rename columns\ncolnames(manual_df) &lt;- c(\"Week\", \"Trump\", \"Harris\", \"Election\")\n\n# Convert Week to Date\nmanual_df$Week &lt;- as.Date(manual_df$Week)\n\n# Replace \"&lt;1\" with 0 and convert to numeric / cleaning data\nmanual_df$Trump    &lt;- as.numeric(gsub(\"&lt;1\", \"0\", manual_df$Trump))\nmanual_df$Harris   &lt;- as.numeric(gsub(\"&lt;1\", \"0\", manual_df$Harris))\nmanual_df$Election &lt;- as.numeric(gsub(\"&lt;1\", \"0\", manual_df$Election))\n\n# Preview\nhead(manual_df)\n\n        Week Trump Harris Election\n1 2024-09-15     7      2        1\n2 2024-09-22     3      2        1\n3 2024-09-29     4      2        1\n4 2024-10-06     4      2        1\n5 2024-10-13     5      3        2\n6 2024-10-20     6      3        3\n\n# Plotting CSV data\nlibrary(ggplot2)\nggplot(manual_df, aes(x = Week)) +\n  geom_line(aes(y = Trump, color = \"Trump\"), linewidth = 1) +\n  geom_line(aes(y = Harris, color = \"Harris\"), linewidth = 1) +\n  geom_line(aes(y = Election, color = \"Election\"), linewidth = 1) +\n  labs(title = \"Google Trends (Manual CSV): Trump, Harris, Election\",\n       x = \"Date\", y = \"Search Interest\") +\n  scale_color_manual(values = c(\"Trump\"=\"red\",\"Harris\"=\"blue\",\"Election\"=\"orange\")) +\n  theme_minimal()"
  },
  {
    "objectID": "assignment02.html#manual-csv-google-trends",
    "href": "assignment02.html#manual-csv-google-trends",
    "title": "Assignment 2: Google Trends Data",
    "section": "",
    "text": "I downloaded the CSV from Google Trends for the terms “Trump, Kamala, Election” (past 12 months).\nThe CSV provides weekly intervals of search interest for each term.\n\n# Load manual CSV\nmanual_df &lt;- read.csv(\"assignment02/TrumpHarrisElection.csv\", skip = 2)\n\n# Rename columns\ncolnames(manual_df) &lt;- c(\"Week\", \"Trump\", \"Harris\", \"Election\")\n\n# Convert Week to Date\nmanual_df$Week &lt;- as.Date(manual_df$Week)\n\n# Replace \"&lt;1\" with 0 and convert to numeric / cleaning data\nmanual_df$Trump    &lt;- as.numeric(gsub(\"&lt;1\", \"0\", manual_df$Trump))\nmanual_df$Harris   &lt;- as.numeric(gsub(\"&lt;1\", \"0\", manual_df$Harris))\nmanual_df$Election &lt;- as.numeric(gsub(\"&lt;1\", \"0\", manual_df$Election))\n\n# Preview\nhead(manual_df)\n\n        Week Trump Harris Election\n1 2024-09-15     7      2        1\n2 2024-09-22     3      2        1\n3 2024-09-29     4      2        1\n4 2024-10-06     4      2        1\n5 2024-10-13     5      3        2\n6 2024-10-20     6      3        3\n\n# Plotting CSV data\nlibrary(ggplot2)\nggplot(manual_df, aes(x = Week)) +\n  geom_line(aes(y = Trump, color = \"Trump\"), linewidth = 1) +\n  geom_line(aes(y = Harris, color = \"Harris\"), linewidth = 1) +\n  geom_line(aes(y = Election, color = \"Election\"), linewidth = 1) +\n  labs(title = \"Google Trends (Manual CSV): Trump, Harris, Election\",\n       x = \"Date\", y = \"Search Interest\") +\n  scale_color_manual(values = c(\"Trump\"=\"red\",\"Harris\"=\"blue\",\"Election\"=\"orange\")) +\n  theme_minimal()"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "assignment4.html",
    "href": "assignment4.html",
    "title": "Assignment 4",
    "section": "",
    "text": "# install.packages(\"tidyverse\")\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# install.packages(\"rvest\")\nlibrary(rvest)\n\n\nAttaching package: 'rvest'\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\n# install.packages(\"stringr\")\nlibrary(stringr)\n\nurl &lt;- 'https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)'\n# Reading the HTML code from the Wiki website\nwiki_gdp &lt;- read_html(url)\nclass(wiki_gdp)\n\n[1] \"xml_document\" \"xml_node\"    \n\ngdp_tables &lt;- wiki_gdp %&gt;%\n  html_nodes(xpath='//*[@id=\"mw-content-text\"]/div[1]/table[2]') %&gt;%\n  html_table()\nclass(gdp_tables)\n\n[1] \"list\"\n\ngdp &lt;- gdp_tables[[1]]\n\nnames(gdp) &lt;- c(\"Rank\", \"Country\", \"GDP_USD\", \"Year\", \"Source\")\n\nWarning: The `value` argument of `names&lt;-()` must have the same length as `x` as of\ntibble 3.0.0.\n\ncolnames(gdp)\n\n[1] \"Rank\"    \"Country\" \"GDP_USD\" \"Year\"   \n\nhead(gdp$Country, n = 10)\n\n [1] \"117,165,394\" \"30,615,743\"  \"19,398,577\"  \"5,013,574\"   \"4,279,828\"  \n [6] \"4,125,213\"   \"3,958,780\"   \"3,361,557\"   \"2,543,677\"   \"2,540,656\"  \n\n## Clean up variables\n## What type is Rank?\n## How about Date? How to fix it?\ngdp$Rank &lt;- as.numeric(gdp$Rank)\n\nWarning: NAs introduced by coercion\n\ngdp$newyear &lt;- str_split_fixed(gdp$Year, \"\\\\[\", n = 2)[, 1]\ngdp$newyear &lt;- trimws(gdp$newyear)\n\ngdp &lt;- gdp[, c(\"Rank\", \"Country\", \"GDP_USD\", \"newyear\")]\n\nwrite.csv(gdp, \"gdp_table.csv\", row.names = FALSE)\n\nprint(gdp, width = Inf)\n\n# A tibble: 222 × 4\n    Rank Country     GDP_USD     newyear    \n   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt;      \n 1    NA 117,165,394 111,326,370 100,834,796\n 2    NA 30,615,743  29,184,890  27,720,700 \n 3    NA 19,398,577  18,743,803  17,794,782 \n 4    NA 5,013,574   4,659,929   4,525,704  \n 5    NA 4,279,828   4,026,211   4,204,495  \n 6    NA 4,125,213   3,912,686   3,575,778  \n 7    NA 3,958,780   3,643,834   3,380,855  \n 8    NA 3,361,557   3,162,079   3,051,832  \n 9    NA 2,543,677   2,372,775   2,300,941  \n10    NA 2,540,656   2,173,836   2,008,419  \n# ℹ 212 more rows"
  },
  {
    "objectID": "assignment03.html",
    "href": "assignment03.html",
    "title": "Assignment 3: Mapping Census Data",
    "section": "",
    "text": "Mapping Census Data\nIn this assignment, I decided to adjust the tidycensus01.R code to look at the labor force dynamics across Michigan counties from ACS. The analysis looks at the size of the civilian labor force and the number of the unemployed population. The analysis identifies the counties with the highest and lowest unemployment rates.\n\n# Packages\nlibrary(tidycensus)\nlibrary(tigris)\n\nTo enable caching of data, set `options(tigris_use_cache = TRUE)`\nin your R script or .Rprofile.\n\nlibrary(sf)\n\nLinking to GEOS 3.13.0, GDAL 3.8.5, PROJ 9.5.1; sf_use_s2() is TRUE\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(readr)\n\noptions(tigris_use_cache = TRUE)\n\n# 1) API key (uncomment and paste your key)\ncensus_api_key(\"991adc56800ae3c63abc4d1f74bc78711191233d\", install = FALSE)\n\nTo install your API key for use in future sessions, run this function with `install = TRUE`.\n\n# 2) Explore variables\nvars &lt;- load_variables(2023, \"acs5\", cache = TRUE)\n\n# 3) Parameters\nstate_abbr &lt;- \"MI\"\ngeo_level  &lt;- \"county\"   # options: state, county, tract, block group\nmy_vars    &lt;- c(labor_force = \"B23025_003\", unemployed = \"B23025_005\")\nyear_acs   &lt;- 2022\nsurvey     &lt;- \"acs5\"\n\n# 4) Download\nacs &lt;- get_acs(\n  geography = geo_level,\n  variables = my_vars,\n  state = state_abbr,\n  year = year_acs,\n  survey = survey,\n  geometry = TRUE\n)\n\nGetting data from the 2018-2022 5-year ACS\n\n# 5) Wide format for convenience\nacs_wide &lt;- acs |&gt;\n  tidyr::pivot_wider(\n    id_cols = c(GEOID, NAME, geometry),\n    names_from = variable,\n    values_from = c(estimate, moe)\n  ) |&gt;\n  # Add computed rate here\n  mutate(\n    unemployment_rate = estimate_unemployed / estimate_labor_force\n  )\n\n\n# 6) Map (Unemployment Rate)\nggplot(acs_wide) +\n  geom_sf(aes(fill = unemployment_rate), color = NA) +\n  scale_fill_viridis_c(name = \"Unemployment Rate\", labels = scales::percent) +\n  labs(\n    title = paste0(\"ACS \", year_acs, \" 5-year: Unemployment Rate — \", state_abbr, \" (\", geo_level, \")\"),\n    caption = \"Source: U.S. Census Bureau via tidycensus\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n# 7) Table (top/bottom by unemployment rate)\ntop10 &lt;- acs_wide |&gt;\n  arrange(desc(unemployment_rate)) |&gt;\n  select(NAME, unemployment_rate, estimate_unemployed, estimate_labor_force) |&gt;\n  slice_head(n = 10)\n\nbottom10 &lt;- acs_wide |&gt;\n  arrange(unemployment_rate) |&gt;\n  select(NAME, unemployment_rate, estimate_unemployed, estimate_labor_force) |&gt;\n  slice_head(n = 10)\n\n\ntop10\n\nSimple feature collection with 10 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -86.6156 ymin: 42.03334 xmax: -82.87024 ymax: 46.77332\nGeodetic CRS:  NAD83\n# A tibble: 10 × 5\n   NAME               unemployment_rate estimate_unemployed estimate_labor_force\n   &lt;chr&gt;                          &lt;dbl&gt;               &lt;dbl&gt;                &lt;dbl&gt;\n 1 Mackinac County, …            0.101                  490                 4839\n 2 Clare County, Mic…            0.0930                1132                12173\n 3 Wayne County, Mic…            0.0897               75008               835963\n 4 Oscoda County, Mi…            0.0889                 270                 3038\n 5 Genesee County, M…            0.0849               16087               189531\n 6 Luce County, Mich…            0.0830                 132                 1591\n 7 Schoolcraft Count…            0.0792                 274                 3460\n 8 Chippewa County, …            0.0770                1310                17021\n 9 Osceola County, M…            0.0766                 748                 9762\n10 Muskegon County, …            0.0746                6217                83299\n# ℹ 1 more variable: geometry &lt;MULTIPOLYGON [°]&gt;\n\nbottom10\n\nSimple feature collection with 10 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -88.13611 ymin: 42.41889 xmax: -83.66481 ymax: 46.24686\nGeodetic CRS:  NAD83\n# A tibble: 10 × 5\n   NAME               unemployment_rate estimate_unemployed estimate_labor_force\n   &lt;chr&gt;                          &lt;dbl&gt;               &lt;dbl&gt;                &lt;dbl&gt;\n 1 Leelanau County, …            0.0321                 322                10042\n 2 Allegan County, M…            0.0337                1991                59038\n 3 Clinton County, M…            0.0370                1511                40804\n 4 Ottawa County, Mi…            0.0378                6024               159492\n 5 Emmet County, Mic…            0.0379                 673                17748\n 6 Charlevoix County…            0.0381                 499                13114\n 7 Dickinson County,…            0.0382                 472                12348\n 8 Grand Traverse Co…            0.0390                1968                50437\n 9 Livingston County…            0.0396                4059               102593\n10 Shiawassee County…            0.0405                1349                33281\n# ℹ 1 more variable: geometry &lt;MULTIPOLYGON [°]&gt;\n\n# 8) Save outputs (optional)\nreadr::write_csv(st_drop_geometry(acs_wide), \"acs_data.csv\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Janette Phan",
    "section": "",
    "text": "I’m Janette Phan, a graduate student in Applied Cognition and Neuroscience at UT Dallas. My background is in Cognitive Science with a focus on cognitive neuroscience and AI, and I’m interested in using data and research to better understand how people think, learn, and interact with technology."
  },
  {
    "objectID": "assignment04.html",
    "href": "assignment04.html",
    "title": "Assignment 4: rvest",
    "section": "",
    "text": "# install.packages(\"tidyverse\")\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# install.packages(\"rvest\")\nlibrary(rvest)\n\n\nAttaching package: 'rvest'\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\n# install.packages(\"stringr\")\nlibrary(stringr)\n\nurl &lt;- 'https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)'\n# Reading the HTML code from the Wiki website\nwiki_gdp &lt;- read_html(url)\nclass(wiki_gdp)\n\n[1] \"xml_document\" \"xml_node\"    \n\ngdp_tables &lt;- wiki_gdp %&gt;%\n  html_nodes(xpath='//*[@id=\"mw-content-text\"]/div[1]/table[2]') %&gt;%\n  html_table()\nclass(gdp_tables)\n\n[1] \"list\"\n\ngdp &lt;- gdp_tables[[1]]\n\nnames(gdp) &lt;- c(\"Rank\", \"Country\", \"GDP_USD\", \"Year\", \"Source\")\n\nWarning: The `value` argument of `names&lt;-()` must have the same length as `x` as of\ntibble 3.0.0.\n\ncolnames(gdp)\n\n[1] \"Rank\"    \"Country\" \"GDP_USD\" \"Year\"   \n\nhead(gdp$Country, n = 10)\n\n [1] \"117,165,394\" \"30,615,743\"  \"19,398,577\"  \"5,013,574\"   \"4,279,828\"  \n [6] \"4,125,213\"   \"3,958,780\"   \"3,361,557\"   \"2,543,677\"   \"2,540,656\"  \n\n## Clean up variables\n## What type is Rank?\n## How about Date? How to fix it?\ngdp$Rank &lt;- as.numeric(gdp$Rank)\n\nWarning: NAs introduced by coercion\n\ngdp$newyear &lt;- str_split_fixed(gdp$Year, \"\\\\[\", n = 2)[, 1]\ngdp$newyear &lt;- trimws(gdp$newyear)\n\ngdp &lt;- gdp[, c(\"Rank\", \"Country\", \"GDP_USD\", \"newyear\")]\n\nwrite.csv(gdp, \"gdp_table.csv\", row.names = FALSE)\n\nprint(gdp, width = Inf)\n\n# A tibble: 222 × 4\n    Rank Country     GDP_USD     newyear    \n   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt;      \n 1    NA 117,165,394 111,326,370 100,834,796\n 2    NA 30,615,743  29,184,890  27,720,700 \n 3    NA 19,398,577  18,743,803  17,794,782 \n 4    NA 5,013,574   4,659,929   4,525,704  \n 5    NA 4,279,828   4,026,211   4,204,495  \n 6    NA 4,125,213   3,912,686   3,575,778  \n 7    NA 3,958,780   3,643,834   3,380,855  \n 8    NA 3,361,557   3,162,079   3,051,832  \n 9    NA 2,543,677   2,372,775   2,300,941  \n10    NA 2,540,656   2,173,836   2,008,419  \n# ℹ 212 more rows"
  },
  {
    "objectID": "assignment05.html",
    "href": "assignment05.html",
    "title": "Assignment 5: Scraping Gov Data",
    "section": "",
    "text": "## Scraping Government data\n## Website: GovInfo (https://www.govinfo.gov/app/search/)\n## Prerequisite: Download from website the list of files to be downloaded\n## Designed for background job\n\n# Start with a clean plate and lean loading to save memory\n \ngc(reset=T)\n\n          used (Mb) gc trigger (Mb) limit (Mb) max used (Mb)\nNcells  608487 32.5    1380721 73.8         NA   608487 32.5\nVcells 1117800  8.6    8388608 64.0      18432  1117800  8.6\n\n# install.packages(c(\"purrr\", \"magrittr\")\nlibrary(purrr)\nlibrary(magrittr) # Alternatively, load tidyverse\n\n\nAttaching package: 'magrittr'\n\n\nThe following object is masked from 'package:purrr':\n\n    set_names\n\n## Set path for reading the listing and home directory\n## For Windows, use \"c:\\\\directory\\\\subdirectory\\\\\"\n## For Mac, \"/Users/YOURNAME/path/\"\n\nsetwd(\"/Users/janettephan/documents/GitHub/phanjanette.github.io/assignment05\")\nlibrary(rjson)\nlibrary(jsonlite)\n\n\nAttaching package: 'jsonlite'\n\n\nThe following objects are masked from 'package:rjson':\n\n    fromJSON, toJSON\n\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\nlibrary(data.table)\n\n\nAttaching package: 'data.table'\n\n\nThe following object is masked from 'package:purrr':\n\n    transpose\n\nlibrary(readr)\n\n## CSV method\ngovfiles= read.csv(file=\"https://github.com/datageneration/datamethods/raw/refs/heads/master/webdata/govinfo-search-results-2024-10-13T07_10_42.csv\", skip=2)\n\n## JSON method\n### rjson\ngf_list &lt;- rjson::fromJSON(file =\"https://github.com/datageneration/datamethods/raw/refs/heads/master/webdata/govinfo-search-results-2024-10-13T07_18_29.json\")\ngovfile2=dplyr::bind_rows(gf_list$resultSet)\n\n### jsonlite\ngf_list1 = jsonlite::read_json(\"https://github.com/datageneration/datamethods/raw/refs/heads/master/webdata/govinfo-search-results-2024-10-13T07_18_29.json\")\n\n### Extract the list\ngovfiles3 &lt;- gf_list1$resultSet\n\n### One more step\ngovfiles3 &lt;- gf_list1$resultSet |&gt; dplyr::bind_rows()\n\n### Sorting publishDate newest to oldest\ngovfiles3 &lt;- govfiles3 |&gt;\n  dplyr::mutate(publishdate = as.Date(publishdate)) |&gt;\n  dplyr::arrange(dplyr::desc(publishdate))\n\n# Preparing for bulk download of government documents\ngovfiles$id = govfiles$packageId\npdf_govfiles_url = govfiles3$pdfLink\npdf_govfiles_id &lt;- govfiles3$index\n\n# Directory to save the pdf's\n# Be sure to create a folder for storing the pdf's\nsave_dir &lt;- \"/Users/janettephan/documents/GitHub/phanjanette.github.io/assignment05\"\n\n# Function to download pdfs\ndownload_govfiles_pdf &lt;- function(url, id) {\n  tryCatch({\n    destfile &lt;- paste0(save_dir, \"govfiles_\", id, \".pdf\")\n    download.file(url, destfile = destfile, mode = \"wb\") # Binary files\n    Sys.sleep(runif(1, 1, 3))  # Important: random sleep between 1 and 3 seconds to avoid suspicion of \"hacking\" the server\n    return(paste(\"Successfully downloaded:\", url))\n  },\n  error = function(e) {\n    return(paste(\"Failed to download:\", url))\n  })\n}\n\n### Download 10 most recent documents\nstart.time &lt;- Sys.time()\nmessage(\"Starting downloads\")\n\nStarting downloads\n\nresults &lt;- 1:10 %&gt;% \n  purrr::map_chr(~ download_govfiles_pdf(pdf_govfiles_url[.], pdf_govfiles_id[.]))\nmessage(\"Finished downloads\")\n\nFinished downloads\n\nend.time &lt;- Sys.time()\ntime.taken &lt;- end.time - start.time\ntime.taken\n\nTime difference of 21.47171 secs\n\nprint(results)\n\n [1] \"Successfully downloaded: https://www.govinfo.gov/content/pkg/BILLS-118sres890is/pdf/BILLS-118sres890is.pdf\"        \n [2] \"Successfully downloaded: https://www.govinfo.gov/content/pkg/BILLS-118sjres114is/pdf/BILLS-118sjres114is.pdf\"      \n [3] \"Successfully downloaded: https://www.govinfo.gov/content/pkg/BILLS-118sres805ats/pdf/BILLS-118sres805ats.pdf\"      \n [4] \"Successfully downloaded: https://www.govinfo.gov/content/pkg/BILLS-118sjres115is/pdf/BILLS-118sjres115is.pdf\"      \n [5] \"Successfully downloaded: https://www.govinfo.gov/content/pkg/BILLS-118sjres113is/pdf/BILLS-118sjres113is.pdf\"      \n [6] \"Successfully downloaded: https://www.govinfo.gov/content/pkg/BILLS-118sjres111is/pdf/BILLS-118sjres111is.pdf\"      \n [7] \"Successfully downloaded: https://www.govinfo.gov/content/pkg/BILLS-118sjres112is/pdf/BILLS-118sjres112is.pdf\"      \n [8] \"Successfully downloaded: https://www.govinfo.gov/content/pkg/CREC-2024-09-25/pdf/CREC-2024-09-25-pt1-PgD951.pdf\"   \n [9] \"Successfully downloaded: https://www.govinfo.gov/content/pkg/CREC-2024-09-25/pdf/CREC-2024-09-25-pt1-PgS6417-2.pdf\"\n[10] \"Successfully downloaded: https://www.govinfo.gov/content/pkg/CREC-2024-09-25/pdf/CREC-2024-09-25-pt1-PgS6418.pdf\""
  },
  {
    "objectID": "assignment05.html#difficulties-encountered",
    "href": "assignment05.html#difficulties-encountered",
    "title": "Assignment 5: Scraping Gov Data",
    "section": "Difficulties Encountered:",
    "text": "Difficulties Encountered:\nI had trouble figuring out the correct date variable because it wasn’t clearly documented in the dataset. Setting up the working directory also took time since I needed to find the right folder where my downloaded files would go and be easy to locate. Another challenge was simply understanding the code itself, as it took a while to learn what each part was doing and how the pieces worked together."
  },
  {
    "objectID": "assignment05.html#usefulness-of-the-scraped-data",
    "href": "assignment05.html#usefulness-of-the-scraped-data",
    "title": "Assignment 5: Scraping Gov Data",
    "section": "Usefulness of the Scraped Data:",
    "text": "Usefulness of the Scraped Data:\nThe scraped data is usable and includes the main information I expected, such as the document title, text, and publication date. After skimming the downloaded documents, they appear accurate, though they would still need closer review to confirm nothing important is missing."
  },
  {
    "objectID": "assignment05.html#how-to-improve",
    "href": "assignment05.html#how-to-improve",
    "title": "Assignment 5: Scraping Gov Data",
    "section": "How to Improve?:",
    "text": "How to Improve?:\nThe scraping process could be improved by using an API instead of downloading files manually. An API would provide cleaner data and make the process faster and more reliable."
  },
  {
    "objectID": "assignment06.html",
    "href": "assignment06.html",
    "title": "Assignment 6: quanteda_textanalytics",
    "section": "",
    "text": "# Sample program for using quanteda for text modeling and analysis\n# Documentation: vignette(\"quickstart\", package = \"quanteda\")\n# Website: https://quanteda.io/\n# Updated: 2025 - Fixed deprecated functions and modernized syntax\n\nlibrary(quanteda)\n\nPackage version: 4.3.1\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: disabled\n\n\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n# Twitter data about President Biden and Xi summit in November 2021\n# Do some background search/study on the event\n\nsummit &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/datamethods/master/textanalytics/summit_11162021.csv\")\n\nRows: 14520 Columns: 90\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (50): screen_name, text, source, reply_to_screen_name, hashtags, symbol...\ndbl  (26): user_id, status_id, display_text_width, reply_to_status_id, reply...\nlgl  (10): is_quote, is_retweet, quote_count, reply_count, ext_media_type, q...\ndttm  (4): created_at, quoted_created_at, retweet_created_at, account_create...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Instead of View() for reproducible scripts, use head() or glimpse()\nhead(summit, 10)\n\n# A tibble: 10 × 90\n   user_id status_id created_at          screen_name     text             source\n     &lt;dbl&gt;     &lt;dbl&gt; &lt;dttm&gt;              &lt;chr&gt;           &lt;chr&gt;            &lt;chr&gt; \n 1 1.38e18   1.46e18 2021-11-16 20:10:23 DSJ78992721     \"Breaking News:… Twitt…\n 2 2.60e 8   1.46e18 2021-11-16 20:10:17 bradhooperarch  \"https://t.co/r… Twitt…\n 3 3.00e 9   1.46e18 2021-11-16 20:10:10 scarecrow1113   \"[Recap] Biden … Twitt…\n 4 3.00e 9   1.46e18 2021-11-15 19:24:04 scarecrow1113   \"U.S. President… Twitt…\n 5 1.36e18   1.46e18 2021-11-16 06:22:29 Internl_Leaks   \"#BREAKING Bide… Twitt…\n 6 1.36e18   1.46e18 2021-11-16 20:09:36 Internl_Leaks   \"#BREAKING Bide… Twitt…\n 7 1.05e18   1.46e18 2021-11-16 20:09:12 Lordsbondserver \"No Breakthroug… Twitt…\n 8 9.55e 8   1.46e18 2021-11-16 20:08:54 KevinCappskj    \"President Bide… Twitt…\n 9 1.26e18   1.46e18 2021-11-16 01:00:05 SayNoToSino     \"Joe Biden and … Twitt…\n10 1.26e18   1.46e18 2021-11-16 20:08:24 SayNoToSino     \"Why did Joe Bi… Twitt…\n# ℹ 84 more variables: display_text_width &lt;dbl&gt;, reply_to_status_id &lt;dbl&gt;,\n#   reply_to_user_id &lt;dbl&gt;, reply_to_screen_name &lt;chr&gt;, is_quote &lt;lgl&gt;,\n#   is_retweet &lt;lgl&gt;, favorite_count &lt;dbl&gt;, retweet_count &lt;dbl&gt;,\n#   quote_count &lt;lgl&gt;, reply_count &lt;lgl&gt;, hashtags &lt;chr&gt;, symbols &lt;chr&gt;,\n#   urls_url &lt;chr&gt;, urls_t.co &lt;chr&gt;, urls_expanded_url &lt;chr&gt;, media_url &lt;chr&gt;,\n#   media_t.co &lt;chr&gt;, media_expanded_url &lt;chr&gt;, media_type &lt;chr&gt;,\n#   ext_media_url &lt;chr&gt;, ext_media_t.co &lt;chr&gt;, ext_media_expanded_url &lt;chr&gt;, …\n\nsum_twt &lt;- summit$text\n\n# Tokenize the text\ntoks &lt;- tokens(sum_twt)\nprint(class(toks))\n\n[1] \"tokens\"\n\n# Create document-feature matrix\nsumtwtdfm &lt;- dfm(toks)\n\n# Latent Semantic Analysis \n# Reference: https://quanteda.io/reference/textmodel_lsa.html\n# NOTE: The 'margin' parameter has been deprecated in recent versions\n# Use textmodel_lsa() without margin parameter for standard LSA\nsum_lsa &lt;- textmodel_lsa(sumtwtdfm, nd = 4)\nsummary(sum_lsa)\n\n                Length    Class     Mode   \nsk                      4 -none-    numeric\ndocs                58080 -none-    numeric\nfeatures            63972 -none-    numeric\nmatrix_low_rank 232218360 -none-    numeric\ndata            232218360 dgCMatrix S4     \n\nhead(sum_lsa$docs)\n\n              [,1]          [,2]          [,3]          [,4]\ntext1 8.678102e-03  9.529008e-03 -3.178574e-03  1.380732e-02\ntext2 8.676818e-06 -8.806186e-06 -5.989637e-06  1.677631e-05\ntext3 2.922127e-03  6.778967e-03  1.131673e-03 -3.176902e-03\ntext4 1.046624e-02  8.884054e-04 -4.282723e-03  4.960680e-03\ntext5 3.251208e-03  8.005843e-03  2.208204e-04 -4.656367e-03\ntext6 3.251208e-03  8.005843e-03  2.208204e-04 -4.656367e-03\n\nprint(class(sum_lsa))\n\n[1] \"textmodel_lsa\"\n\n# Intrepretation of LSA results\n# LSA reduced your high-dimensional space (63,972 features) into \n# 4 latent semantic dimensions while preserving most of the information.\n\n# What Each Column Represents:\n# **Column 1-4**: The 4 latent semantic dimensions\n# **Each row** (text1-text6): A document's position in semantic space\n# **Cell values**: The document's score/loading on that dimension\n\n### Interpretation:\n# | Aspect | Meaning |\n#  |--------|---------|\n#  | **Large positive values** | Document strongly represents that semantic concept |\n#  | **Large negative values** | Document's content opposes that semantic concept |\n#  | **Values near zero** | Document barely relates to that dimension |\n# | **Text2 row** | Very small values overall = document is semantically \"neutral\" or sparse |\n#  | **Text4 row** | Larger values = rich semantic content across dimensions |\n\n\n# \n\n# ============================================================================\n# HASHTAG ANALYSIS\n# ============================================================================\n\n# Create DFM with punctuation removed\ntweet_dfm &lt;- tokens(sum_twt, remove_punct = TRUE) |&gt;\n  dfm()\nhead(tweet_dfm)\n\nDocument-feature matrix of: 6 documents, 15,927 features (99.89% sparse) and 0 docvars.\n       features\ndocs    breaking news us president biden amp communist china leader xi\n  text1        1    1  1         1     1   1         1     2      1  1\n  text2        0    0  0         0     0   0         0     0      0  0\n  text3        0    0  0         0     1   0         0     0      0  1\n  text4        0    0  0         1     1   0         0     0      0  1\n  text5        0    0  0         0     1   0         0     0      0  1\n  text6        0    0  0         0     1   0         0     0      0  1\n[ reached max_nfeat ... 15,917 more features ]\n\n# Select hashtags (pattern starting with #)\ntag_dfm &lt;- dfm_select(tweet_dfm, pattern = \"#*\")\ntoptag &lt;- names(topfeatures(tag_dfm, 50))\nhead(toptag, 10)\n\n [1] \"#china\"       \"#biden\"       \"#xijinping\"   \"#joebiden\"    \"#america\"    \n [6] \"#americans\"   \"#coronavirus\" \"#fentanyl\"    \"#xi\"          \"#us\"         \n\n# Feature co-occurrence matrix for hashtags\ntag_fcm &lt;- fcm(tag_dfm)\nhead(tag_fcm)\n\nFeature co-occurrence matrix of: 6 by 665 features.\n               features\nfeatures        #breaking #breakingnews #biden #china #usa #pray4america\n  #breaking             0             4      5      5    5             0\n  #breakingnews         0             0      4      5    4             0\n  #biden                0             0      0    443   49             0\n  #china                0             0      0      8   76             0\n  #usa                  0             0      0      0    6             0\n  #pray4america         0             0      0      0    0             0\n               features\nfeatures        #joebiden #xijinping #america #americans\n  #breaking             0          0        0          0\n  #breakingnews         0          0        0          0\n  #biden              299        370      302        295\n  #china              339        434      308        295\n  #usa                 12         15        0          0\n  #pray4america         0          0        0          0\n[ reached max_nfeat ... 655 more features ]\n\n# Select top hashtags for visualization\n# Updated: fcm_select() is still valid but ensure compatibility\ntopgat_fcm &lt;- fcm_select(tag_fcm, pattern = toptag)\n\n# Create network plot with improved parameters\ntextplot_network(\n  topgat_fcm, \n  min_freq = 50, \n  edge_alpha = 0.8, \n  edge_size = 1,\n  vertex_size = 3\n)\n\n\n\n\n\n\n\n# ============================================================================\n# USER MENTION ANALYSIS\n# ============================================================================\n\n# Select user mentions (pattern starting with @)\nuser_dfm &lt;- dfm_select(tweet_dfm, pattern = \"@*\")\ntopuser &lt;- names(topfeatures(user_dfm, 50))\nhead(topuser, 20)\n\n [1] \"@potus\"           \"@politico\"        \"@joebiden\"        \"@jendeben\"       \n [5] \"@eneskanter\"      \"@nwadhams\"        \"@phelimkine\"      \"@nahaltoosi\"     \n [9] \"@nba\"             \"@washwizards\"     \"@pelicansnba\"     \"@capitalonearena\"\n[13] \"@kevinliptakcnn\"  \"@foxbusiness\"     \"@morningsmaria\"   \"@scmpnews\"       \n[17] \"@petermartin_pcm\" \"@nytimes\"         \"@uyghur_american\" \"@kaylatausche\"   \n\n# Feature co-occurrence matrix for users\nuser_fcm &lt;- fcm(user_dfm)\nhead(user_fcm, 20)\n\nFeature co-occurrence matrix of: 20 by 711 features.\n                 features\nfeatures          @youtube @bfmtv @cnn @lauhaim @barackobama @joebiden\n  @youtube               0      0    0        0            0         0\n  @bfmtv                 0      0    1        1            1         1\n  @cnn                   0      0    0        1            1         1\n  @lauhaim               0      0    0        0            1         1\n  @barackobama           0      0    0        0            0         1\n  @joebiden              0      0    0        0            0         3\n  @kamalaharris          0      0    0        0            0         0\n  @hillaryclinton        0      0    0        0            0         0\n  @billclinton           0      0    0        0            0         0\n  @cbsnews               0      0    0        0            0         0\n                 features\nfeatures          @kamalaharris @hillaryclinton @billclinton @cbsnews\n  @youtube                    0               0            0        0\n  @bfmtv                      1               1            1        1\n  @cnn                        1               1            1        1\n  @lauhaim                    1               1            1        1\n  @barackobama                1               1            1        1\n  @joebiden                   1               1            1        1\n  @kamalaharris               0               1            1        1\n  @hillaryclinton             0               0            1        1\n  @billclinton                0               0            0        1\n  @cbsnews                    0               0            0        0\n[ reached max_nfeat ... 10 more features, reached max_nfeat ... 701 more features ]\n\n# Select top users for visualization\nuser_fcm &lt;- fcm_select(user_fcm, pattern = topuser)\n\n# Create network plot for user mentions\ntextplot_network(\n  user_fcm, \n  min_freq = 20, \n  edge_color = \"firebrick\", \n  edge_alpha = 0.8, \n  edge_size = 1,\n  vertex_size = 3\n)\n\n\n\n\n\n\n\n# ============================================================================\n# ADDITIONAL ANALYSIS OPTIONS (for reference)\n# ============================================================================\n\n# Optional: Keyword frequency analysis\ntop_features &lt;- topfeatures(tweet_dfm, 30)\nprint(top_features)\n\n    biden       the        xi    summit        to       and   virtual         a \n    12338     12274     12027     11167     10456      9317      7483      6384 \npresident      with   jinping        of        on       for        in     china \n     6148      5388      4766      4564      4498      3589      3359      3011 \n  chinese       joe    monday        is      hold        as        us      will \n     2716      2598      2540      2350      2301      2256      2179      2093 \n       at      that      have       his    taiwan  biden-xi \n     2041      2004      1947      1817      1578      1556 \n\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ stringr   1.5.2\n✔ forcats   1.0.1     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Get top features with frequencies\ntop_freq &lt;- topfeatures(tweet_dfm, 30)\n\n# Convert to dataframe for ggplot\nfreq_df &lt;- tibble(\n  word = names(top_freq),\n  frequency = as.numeric(top_freq)\n) %&gt;%\n  mutate(word = fct_reorder(word, frequency))\n\n# Create frequency plot\nggplot(freq_df, aes(x = frequency, y = word, fill = frequency)) +\n  geom_col(show.legend = FALSE) +\n  scale_fill_gradient(low = \"lightblue\", high = \"darkblue\") +\n  labs(\n    title = \"Top 30 Most Frequent Terms\",\n    subtitle = \"Biden-Xi Summit Tweets\",\n    x = \"Frequency\",\n    y = \"Term\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text = element_text(size = 11),\n    title = element_text(size = 13, face = \"bold\")\n  )"
  },
  {
    "objectID": "assignment06.html#quanteda_textanalytics02a.r",
    "href": "assignment06.html#quanteda_textanalytics02a.r",
    "title": "Assignment 6: quanteda_textanalytics",
    "section": "quanteda_textanalytics02a.R",
    "text": "quanteda_textanalytics02a.R\n\n# Quanteda Text Statistics and Analysis Workshop\n# Advanced text analysis using US Presidential Inaugural Addresses\n# Documentation: https://quanteda.io/\n# Updated: 2025 - Fixed issues and modernized code\n\nlibrary(quanteda)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(quanteda.textstats)  # Add textstats library at top\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n# Set seed for reproducibility\nset.seed(100)\n\n# ============================================================================\n# SECTION 1: WORDCLOUD - EARLY US PRESIDENTIAL SPEECHES (1789-1826)\n# ============================================================================\n\ncat(\"\\n=== Creating wordcloud for historical inaugural speeches ===\\n\")\n\n\n=== Creating wordcloud for historical inaugural speeches ===\n\n# Create DFM from speeches before 1826\ndfm_inaug &lt;- corpus_subset(data_corpus_inaugural, Year &lt;= 1826) |&gt;\n  tokens(remove_punct = TRUE) |&gt;\n  tokens_remove(stopwords(\"english\")) |&gt;\n  dfm() |&gt;\n  dfm_trim(min_termfreq = 10, verbose = FALSE)\n\n# Generate wordcloud\ntextplot_wordcloud(dfm_inaug, max_words = 100)\n\n\n\n\n\n\n\n# ============================================================================\n# SECTION 2: COMPARISON WORDCLOUD - RECENT PRESIDENTS\n# ============================================================================\n\ncat(\"\\n=== Creating comparison wordcloud (Trump, Obama, Bush) ===\\n\")\n\n\n=== Creating comparison wordcloud (Trump, Obama, Bush) ===\n\n# Create comparison wordcloud for three presidents\ncomparison_wc &lt;- corpus_subset(data_corpus_inaugural, \n                               President %in% c(\"Trump\", \"Obama\", \"Bush\")) |&gt;\n  tokens(remove_punct = TRUE) |&gt;\n  tokens_remove(stopwords(\"english\")) |&gt;\n  dfm() |&gt;\n  dfm_group(groups = President) |&gt;\n  dfm_trim(min_termfreq = 5, verbose = FALSE)\n\n# Plot comparison wordcloud\ntextplot_wordcloud(comparison_wc, comparison = TRUE, max_words = 50)\n\n\n\n\n\n\n\n# ============================================================================\n# SECTION 3: CUSTOM COLORED WORDCLOUD\n# ============================================================================\n\ncat(\"\\n=== Creating custom colored wordcloud ===\\n\")\n\n\n=== Creating custom colored wordcloud ===\n\ntextplot_wordcloud(\n  dfm_inaug, \n  min_count = 10,\n  max_words = 80,\n  color = c(\"red\", \"pink\", \"green\", \"purple\", \"orange\", \"blue\")\n)\n\n\n\n\n\n\n\n# ============================================================================\n# SECTION 4: X-RAY PLOT - TEMPORAL KEYWORD ANALYSIS\n# ============================================================================\n\ncat(\"\\n=== Creating X-ray plots for keyword temporal distribution ===\\n\")\n\n\n=== Creating X-ray plots for keyword temporal distribution ===\n\n# Subset speeches from 1950 onwards\ndata_corpus_inaugural_subset &lt;- \n  corpus_subset(data_corpus_inaugural, Year &gt; 1949)\n\n# Single keyword x-ray plot: \"american\"\ncat(\"Analyzing term: 'american'\\n\")\n\nAnalyzing term: 'american'\n\nkwic_american &lt;- kwic(tokens(data_corpus_inaugural_subset), \n                      pattern = \"american\")\n\ntextplot_xray(kwic_american)\n\n\n\n\n\n\n\n# ──────────────────────────────────────────────────────────────────────────\n# INTERPRETATION GUIDE: X-ray plot for \"american\"\n# ──────────────────────────────────────────────────────────────────────────\n# \n# What to look for:\n# - PEAKS: Years with high frequency of \"american\" usage\n#   * Likely during periods of national identity emphasis\n#   * Post-war speeches often emphasize national identity\n#   * Cold War speeches reference \"American values\"\n#\n# - TROUGHS: Years with low or no usage\n#   * Some presidents may use alternative phrasing\n#   * Different rhetorical strategies\n#\n# - HISTORICAL CONTEXT:\n#   * 1950s-1960s: Cold War peak (anti-communist rhetoric)\n#   * 1980s: Reagan's \"American exceptionalism\" themes\n#   * 1990s-2000s: Post-Cold War identity reconstruction\n#   * 2010s: Obama/Trump shifts in national identity framing\n# ──────────────────────────────────────────────────────────────────────────\n\n# ============================================================================\n# SECTION 5: MULTI-KEYWORD X-RAY COMPARISON\n# ============================================================================\n\ncat(\"\\n=== Creating multi-keyword X-ray plots ===\\n\")\n\n\n=== Creating multi-keyword X-ray plots ===\n\n# Extract keywords for multiple terms\nkwic_american &lt;- kwic(tokens(data_corpus_inaugural_subset), \n                      pattern = \"american\")\nkwic_people &lt;- kwic(tokens(data_corpus_inaugural_subset), \n                    pattern = \"people\")\nkwic_communist &lt;- kwic(tokens(data_corpus_inaugural_subset), \n                       pattern = \"communist\")\n\n# Plot all three keywords\ntextplot_xray(\n  kwic_american,\n  kwic_people,\n  kwic_communist\n)\n\n\n\n\n\n\n\n# ──────────────────────────────────────────────────────────────────────────\n# INTERPRETATION: Why is \"communist\" missing?\n# ──────────────────────────────────────────────────────────────────────────\n# \n# The \"communist\" plot appears empty because:\n# - Term has no occurrences after 1949 in the selected speeches, OR\n# - The term appears infrequently/inconsistently\n# \n# This makes historical sense:\n# - Cold War rhetoric (1950s-1980s) did use \"communist\" more frequently\n# - But it may appear in context clauses, not as main keyword\n# - Term usage declined after Cold War (post-1991)\n# - Modern speeches may use different terminology (\"authoritarian\", \"hostile\")\n# ──────────────────────────────────────────────────────────────────────────\n\n# ============================================================================\n# SECTION 6: CUSTOMIZED MULTI-KEYWORD X-RAY WITH STYLING\n# ============================================================================\n\ncat(\"\\n=== Creating styled multi-keyword X-ray plot ===\\n\")\n\n\n=== Creating styled multi-keyword X-ray plot ===\n\n# Create the x-ray plot\ng &lt;- textplot_xray(\n  kwic_american,\n  kwic_people,\n  kwic_communist\n)\n\n# Add custom styling with ggplot2\ng_styled &lt;- g + \n  aes(color = keyword) + \n  scale_color_manual(\n    values = c(\"american\" = \"blue\", \"people\" = \"red\", \"communist\" = \"green\"),\n    breaks = c(\"american\", \"people\", \"communist\"),\n    na.value = \"gray80\"\n  ) +\n  labs(\n    title = \"Temporal Distribution of Key Terms in Presidential Speeches\",\n    subtitle = \"Post-1949 Inaugural Addresses\",\n    x = \"Year\",\n    y = \"Occurrence\",\n    color = \"Keyword\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    plot.title = element_text(face = \"bold\", size = 14)\n  )\n\nprint(g_styled)\n\n\n\n\n\n\n\n# ============================================================================\n# SECTION 7: FREQUENCY ANALYSIS - BASIC\n# ============================================================================\n\ncat(\"\\n=== Analyzing term frequencies ===\\n\")\n\n\n=== Analyzing term frequencies ===\n\n# Get top 100 features with frequency statistics\nfeatures_dfm_inaug &lt;- textstat_frequency(dfm_inaug, n = 100)\n\n# Sort by frequency (descending)\nfeatures_dfm_inaug &lt;- features_dfm_inaug |&gt;\n  mutate(feature = reorder(feature, -frequency))\n\n# Plot frequency distribution\nggplot(features_dfm_inaug, aes(x = feature, y = frequency)) +\n  geom_point(color = \"steelblue\", size = 2) +\n  geom_line(color = \"steelblue\", alpha = 0.3, group = 1) +\n  labs(\n    title = \"Top 100 Terms: Historical Inaugural Speeches (1789-1826)\",\n    x = \"Term\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5, size = 8),\n    plot.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n# ============================================================================\n# SECTION 8: FREQUENCY GROUPED BY PRESIDENT\n# ============================================================================\n\ncat(\"\\n=== Analyzing frequency by president ===\\n\")\n\n\n=== Analyzing frequency by president ===\n\n# Create DFM from recent speeches\ndata_corpus_recent &lt;- corpus_subset(data_corpus_inaugural, Year &gt; 1949)\n\n# Calculate frequencies grouped by president\nfreq_by_pres &lt;- dfm(tokens(data_corpus_recent, remove_punct = TRUE)) |&gt;\n  dfm_trim(min_termfreq = 2) |&gt;\n  textstat_frequency(groups = docvars(data_corpus_recent, \"President\"))\n\n# Filter for specific term: \"american\"\nfreq_american &lt;- freq_by_pres |&gt;\n  filter(feature %in% \"american\")\n\n# Plot frequency of \"american\" by president\nggplot(freq_american, aes(x = group, y = frequency)) +\n  geom_point(size = 3, color = \"darkblue\") +\n  geom_line(color = \"darkblue\", alpha = 0.5, group = 1) +\n    labs(\n    title = 'Frequency of Term \"american\" by President',\n    subtitle = \"Post-1949 Inaugural Speeches\",\n    x = \"President\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    plot.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n# ============================================================================\n# SECTION 9: RELATIVE FREQUENCY ANALYSIS\n# ============================================================================\n\ncat(\"\\n=== Analyzing relative frequency (normalized) ===\\n\")\n\n\n=== Analyzing relative frequency (normalized) ===\n\n# Create DFM with proportional weighting\ndfm_rel_freq &lt;- dfm(tokens(data_corpus_recent, remove_punct = TRUE)) |&gt;\n  dfm_trim(min_termfreq = 2) |&gt;\n  dfm_weight(scheme = \"prop\") |&gt;\n  (\\(x) x * 100)()  # Convert to percentages\n\n# Verify structure\nhead(dfm_rel_freq)\n\nDocument-feature matrix of: 6 documents, 2,273 features (77.60% sparse) and 4 docvars.\n                 features\ndocs                      my    friends    before          i      begin\n  1953-Eisenhower 0.18058691 0.18058691 0.2257336 0.13544018 0.04514673\n  1957-Eisenhower 0.25706941 0.12853470 0.1928021 0.06426735 0         \n  1961-Kennedy    0.23237800 0.07745933 0.1549187 0.38729667 0.23237800\n  1965-Johnson    0.21276596 0.07092199 0.2836879 1.06382979 0         \n  1969-Nixon      0.34912718 0          0.1496259 1.04738155 0         \n  1973-Nixon      0.05714286 0.05714286 0.2285714 0.68571429 0         \n                 features\ndocs                   the expression       of     those   thoughts\n  1953-Eisenhower 7.720090 0.04514673 6.410835 0.1805869 0.04514673\n  1957-Eisenhower 7.326478 0          6.169666 0.1928021 0         \n  1961-Kennedy    6.661503 0          5.034857 0.5422153 0         \n  1965-Johnson    5.460993 0          4.042553 0.2127660 0         \n  1969-Nixon      6.783042 0          4.688279 0.5486284 0.04987531\n  1973-Nixon      4.742857 0          3.885714 0.3428571 0         \n[ reached max_nfeat ... 2,263 more features ]\n\n# Calculate relative frequencies grouped by president\nrel_freq &lt;- textstat_frequency(\n  dfm_rel_freq, \n  groups = docvars(data_corpus_recent, \"President\")\n)\n\n# Filter for \"american\"\nrel_freq_american &lt;- rel_freq |&gt;\n  filter(feature %in% \"american\")\n\n# Plot relative frequency\nggplot(rel_freq_american, aes(x = group, y = frequency)) +\n  geom_point(size = 3, color = \"coral\") +\n  geom_line(color = \"coral\", alpha = 0.5, group = 1) +\n  labs(\n    title = 'Relative Frequency of \"american\" by President (%)',\n    subtitle = \"Normalized by document length\",\n    x = \"President\",\n    y = \"Relative Frequency (%)\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    plot.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n# ============================================================================\n# SECTION 10: TOP TERMS BY PRESIDENT (FACETED VIEW)\n# ============================================================================\n\ncat(\"\\n=== Creating faceted view of top terms by president ===\\n\")\n\n\n=== Creating faceted view of top terms by president ===\n\n# Create DFM for recent presidents (2000 onwards)\ndfm_weight_pres &lt;- corpus_subset(data_corpus_inaugural, Year &gt; 2000) |&gt;\n  tokens(remove_punct = TRUE) |&gt;\n  tokens_remove(stopwords(\"english\")) |&gt;\n  dfm() |&gt;\n  dfm_weight(scheme = \"prop\")\n\n# Get top 10 terms for each president\nfreq_weight &lt;- textstat_frequency(\n  dfm_weight_pres,\n  n = 10,\n  groups = docvars(corpus_subset(data_corpus_inaugural, Year &gt; 2000), \"President\")\n)\n\n# Create faceted plot\nggplot(\n  data = freq_weight,\n  aes(x = reorder(feature, frequency), y = frequency)\n) +\n  geom_col(fill = \"steelblue\", alpha = 0.8) +\n  facet_wrap(~group, scales = \"free_x\", ncol = 2) +\n  coord_flip() +\n  labs(\n    title = \"Top 10 Terms by President\",\n    subtitle = \"Post-2000 Inaugural Speeches (Relative Frequency)\",\n    x = \"Term\",\n    y = \"Relative Frequency\"\n  ) +\n  theme_minimal() +\n  theme(\n    strip.text = element_text(face = \"bold\", size = 11),\n    plot.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n# ============================================================================\n# SECTION 11: KEYNESS ANALYSIS - COMPARING TWO PRESIDENTS\n# ============================================================================\n\ncat(\"\\n=== Analyzing keyness: Trump vs Obama ===\\n\")\n\n\n=== Analyzing keyness: Trump vs Obama ===\n\n# Create corpus for just Obama and Trump\npres_corpus &lt;- corpus_subset(\n  data_corpus_inaugural,\n  President %in% c(\"Obama\", \"Trump\")\n)\n\n# Create DFM grouped by president\npres_dfm &lt;- tokens(pres_corpus, remove_punct = TRUE) |&gt;\n  tokens_remove(stopwords(\"english\")) |&gt;\n  tokens_group(groups = President) |&gt;\n  dfm()\n\n# Calculate keyness (Trump is target group)\nresult_keyness &lt;- textstat_keyness(pres_dfm, target = \"Trump\")\n\n# Display keyness statistics\nprint(head(result_keyness, 15))\n\n     feature      chi2            p n_target n_reference\n1      thank 18.811939 1.442613e-05       26           3\n2       back 11.206409 8.151529e-04       16           2\n3    america 10.224667 1.385747e-03       36          14\n4      bring  8.550628 3.454030e-03       11           1\n5   american  8.372065 3.810325e-03       24           8\n6    country  8.372065 3.810325e-03       24           8\n7      right  7.557101 5.977407e-03       10           1\n8      great  7.467001 6.284013e-03       16           4\n9  president  7.359631 6.670448e-03       14           3\n10     going  7.281149 6.968194e-03        9           0\n11     never  6.610632 1.013717e-02       15           4\n12   foreign  6.272149 1.226503e-02        8           0\n13    dreams  5.267586 2.172612e-02        7           0\n14     first  5.267586 2.172612e-02        7           0\n15  millions  5.267586 2.172612e-02        7           0\n\n# ──────────────────────────────────────────────────────────────────────────\n# Plot 1: Keyness with reference (Obama) shown on left\n# ──────────────────────────────────────────────────────────────────────────\n\ntextplot_keyness(result_keyness, show_reference = TRUE)\n\n\n\n\n\n\n\n# ──────────────────────────────────────────────────────────────────────────\n# Plot 2: Keyness without reference (cleaner look)\n# ──────────────────────────────────────────────────────────────────────────\n\ntextplot_keyness(result_keyness, show_reference = FALSE)\n\n\n\n\n\n\n\n# ──────────────────────────────────────────────────────────────────────────\n# INTERPRETATION: Understanding Keyness\n# ──────────────────────────────────────────────────────────────────────────\n#\n# KEYNESS measures statistical association of terms with one document group\n# compared to another (target vs. reference).\n#\n# POSITIVE KEYNESS (right side, Trump's distinctive terms):\n# - Terms more associated with Trump speeches\n# - Higher frequency relative to Obama\n# - Example: \"American first\" rhetoric, business terminology\n#\n# NEGATIVE KEYNESS (left side, Obama's distinctive terms):\n# - Terms more associated with Obama speeches\n# - Higher frequency relative to Trump\n# - Example: \"Change\", \"hope\", multi-cultural references\n#\n# CHI-SQUARED TEST: Measures whether frequency differences are significant\n# - Larger chi-squared values = stronger association\n# - Statistical significance indicates real difference, not random variation\n#\n# ──────────────────────────────────────────────────────────────────────────\n\n# ============================================================================\n# OPTIONAL: SUMMARY STATISTICS\n# ============================================================================\n\ncat(\"\\n=== SUMMARY: Data Corpus Overview ===\\n\")\n\n\n=== SUMMARY: Data Corpus Overview ===\n\ncat(\"Total speeches in inaugural corpus:\", ndoc(data_corpus_inaugural), \"\\n\")\n\nTotal speeches in inaugural corpus: 60 \n\ncat(\"Time period:\", min(docvars(data_corpus_inaugural, \"Year\")),\n    \"-\", max(docvars(data_corpus_inaugural, \"Year\")), \"\\n\")\n\nTime period: 1789 - 2025 \n\ncat(\"Number of unique presidents:\", length(unique(docvars(data_corpus_inaugural, \"President\"))), \"\\n\")\n\nNumber of unique presidents: 36 \n\ncat(\"\\n\")"
  },
  {
    "objectID": "assignment06.html#analysis",
    "href": "assignment06.html#analysis",
    "title": "Assignment 6: quanteda_textanalytics",
    "section": "Analysis",
    "text": "Analysis"
  },
  {
    "objectID": "assignment06.html#what-is-wordfish",
    "href": "assignment06.html#what-is-wordfish",
    "title": "Assignment 6: quanteda_textanalytics",
    "section": "What is Wordfish?",
    "text": "What is Wordfish?"
  }
]
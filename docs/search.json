[
  {
    "objectID": "reviews.html",
    "href": "reviews.html",
    "title": "Reviews",
    "section": "",
    "text": "Podcast Review: The Next Industrial Revolution is Industrial AI\nTo get ready for class 2, I listened to a recent Dataframed podcast episode called “The Next Industrial Revolution is Industrial AI” with Barbara Humpton (CEO, Siemens USA) and Olympia Brikis (Director of Industrial AI at Siemens USA).\nThe general theme was industrial AI versus consumer AI. Consumer AI is found in ordinary life, like a recommendation feature or a chatbot on a website. It doesn’t matter when it makes mistakes because there is little risk and is not as strict. Industrial AI, on the other hand, runs inside factories, power generation, and other mechanical processes. It has to be extremely safe and trustworthy because little mistakes can destroy a factory line or pose safety risks.\nWhat is most interesting is that industrial AI is being used not to replace humans but to help them. The speakers explained how technologies like predictive maintenance, computer-vision quality checks, and digital twins (virtual copies of a manufacturing line) allow factories to work better. For example, a digital twin can simulate a new line before it’s ever built, cutting setup time from years to months. A different story illustrated how computer vision systems of today inspect each car door in real-time as opposed to spot checks, which provides assurance of quality at a much greater scale. The show also brought out the “human + machine” partnership. Instead of laying off individuals, companies have the same workers but use them more efficiently. Workers depart from monotonous manual tasks, like inspecting every part manually, to exceptions and process optimization. I liked this because it showed a better future for work, especially in light of the specter that AI is going to take people’s jobs away. AI as a means to upskill, not fear.\nAlthough, I would have liked to see more numbers and metrics. They mentioned timelines and substantial productivity gains, but it would have been nice to see specific metrics. I also would have liked to hear more about concerns like safety risks, data quality, or model learning in extreme factory environments.\nOverall, it is a great primer for those who want to know how AI is already changing manufacturing. It is especially helpful for plant managers, engineers, and policymakers thinking about workforce development. For me, and especially for one who is open to adopting AI as new technology, the most important takeaway is that industrial AI is not about substituting humans, but it’s about humans and AI working together to make industries smarter, faster, and safer."
  },
  {
    "objectID": "assignment01.html",
    "href": "assignment01.html",
    "title": "Assignment 1",
    "section": "",
    "text": "This website is created using RStudio Quarto with the Cosmo theme for a clean and professional look. The navigation bar includes multiple tabs that connect to different areas of content, such as the Home page, Assignments for EPPS 6302: Methods of Data Collection and Production, and my CV/Resume.\nWhile it is currently designed to organize and showcase course assignments, the site will gradually develop into a portfolio of projects that reflects my academic and professional interests."
  },
  {
    "objectID": "assignment02.html",
    "href": "assignment02.html",
    "title": "Assignment 2: Google Trends Data",
    "section": "",
    "text": "I downloaded the CSV from Google Trends for the terms “Trump, Kamala, Election” (past 12 months).\nThe CSV provides weekly intervals of search interest for each term.\n\n# Load manual CSV\nmanual_df &lt;- read.csv(\"assignment02/TrumpHarrisElection.csv\", skip = 2)\n\n# Rename columns\ncolnames(manual_df) &lt;- c(\"Week\", \"Trump\", \"Harris\", \"Election\")\n\n# Convert Week to Date\nmanual_df$Week &lt;- as.Date(manual_df$Week)\n\n# Replace \"&lt;1\" with 0 and convert to numeric / cleaning data\nmanual_df$Trump    &lt;- as.numeric(gsub(\"&lt;1\", \"0\", manual_df$Trump))\nmanual_df$Harris   &lt;- as.numeric(gsub(\"&lt;1\", \"0\", manual_df$Harris))\nmanual_df$Election &lt;- as.numeric(gsub(\"&lt;1\", \"0\", manual_df$Election))\n\n# Preview\nhead(manual_df)\n\n        Week Trump Harris Election\n1 2024-09-15     7      2        1\n2 2024-09-22     3      2        1\n3 2024-09-29     4      2        1\n4 2024-10-06     4      2        1\n5 2024-10-13     5      3        2\n6 2024-10-20     6      3        3\n\n# Plotting CSV data\nlibrary(ggplot2)\nggplot(manual_df, aes(x = Week)) +\n  geom_line(aes(y = Trump, color = \"Trump\"), linewidth = 1) +\n  geom_line(aes(y = Harris, color = \"Harris\"), linewidth = 1) +\n  geom_line(aes(y = Election, color = \"Election\"), linewidth = 1) +\n  labs(title = \"Google Trends (Manual CSV): Trump, Harris, Election\",\n       x = \"Date\", y = \"Search Interest\") +\n  scale_color_manual(values = c(\"Trump\"=\"red\",\"Harris\"=\"blue\",\"Election\"=\"orange\")) +\n  theme_minimal()"
  },
  {
    "objectID": "assignment02.html#manual-csv-google-trends",
    "href": "assignment02.html#manual-csv-google-trends",
    "title": "Assignment 2: Google Trends Data",
    "section": "",
    "text": "I downloaded the CSV from Google Trends for the terms “Trump, Kamala, Election” (past 12 months).\nThe CSV provides weekly intervals of search interest for each term.\n\n# Load manual CSV\nmanual_df &lt;- read.csv(\"assignment02/TrumpHarrisElection.csv\", skip = 2)\n\n# Rename columns\ncolnames(manual_df) &lt;- c(\"Week\", \"Trump\", \"Harris\", \"Election\")\n\n# Convert Week to Date\nmanual_df$Week &lt;- as.Date(manual_df$Week)\n\n# Replace \"&lt;1\" with 0 and convert to numeric / cleaning data\nmanual_df$Trump    &lt;- as.numeric(gsub(\"&lt;1\", \"0\", manual_df$Trump))\nmanual_df$Harris   &lt;- as.numeric(gsub(\"&lt;1\", \"0\", manual_df$Harris))\nmanual_df$Election &lt;- as.numeric(gsub(\"&lt;1\", \"0\", manual_df$Election))\n\n# Preview\nhead(manual_df)\n\n        Week Trump Harris Election\n1 2024-09-15     7      2        1\n2 2024-09-22     3      2        1\n3 2024-09-29     4      2        1\n4 2024-10-06     4      2        1\n5 2024-10-13     5      3        2\n6 2024-10-20     6      3        3\n\n# Plotting CSV data\nlibrary(ggplot2)\nggplot(manual_df, aes(x = Week)) +\n  geom_line(aes(y = Trump, color = \"Trump\"), linewidth = 1) +\n  geom_line(aes(y = Harris, color = \"Harris\"), linewidth = 1) +\n  geom_line(aes(y = Election, color = \"Election\"), linewidth = 1) +\n  labs(title = \"Google Trends (Manual CSV): Trump, Harris, Election\",\n       x = \"Date\", y = \"Search Interest\") +\n  scale_color_manual(values = c(\"Trump\"=\"red\",\"Harris\"=\"blue\",\"Election\"=\"orange\")) +\n  theme_minimal()"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "assignment4.html",
    "href": "assignment4.html",
    "title": "Assignment 4",
    "section": "",
    "text": "# install.packages(\"tidyverse\")\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# install.packages(\"rvest\")\nlibrary(rvest)\n\n\nAttaching package: 'rvest'\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\n# install.packages(\"stringr\")\nlibrary(stringr)\n\nurl &lt;- 'https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)'\n# Reading the HTML code from the Wiki website\nwiki_gdp &lt;- read_html(url)\nclass(wiki_gdp)\n\n[1] \"xml_document\" \"xml_node\"    \n\ngdp_tables &lt;- wiki_gdp %&gt;%\n  html_nodes(xpath='//*[@id=\"mw-content-text\"]/div[1]/table[2]') %&gt;%\n  html_table()\nclass(gdp_tables)\n\n[1] \"list\"\n\ngdp &lt;- gdp_tables[[1]]\n\nnames(gdp) &lt;- c(\"Rank\", \"Country\", \"GDP_USD\", \"Year\", \"Source\")\n\nWarning: The `value` argument of `names&lt;-()` must have the same length as `x` as of\ntibble 3.0.0.\n\ncolnames(gdp)\n\n[1] \"Rank\"    \"Country\" \"GDP_USD\" \"Year\"   \n\nhead(gdp$Country, n = 10)\n\n [1] \"117,165,394\" \"30,615,743\"  \"19,398,577\"  \"5,013,574\"   \"4,279,828\"  \n [6] \"4,125,213\"   \"3,958,780\"   \"3,361,557\"   \"2,543,677\"   \"2,540,656\"  \n\n## Clean up variables\n## What type is Rank?\n## How about Date? How to fix it?\ngdp$Rank &lt;- as.numeric(gdp$Rank)\n\nWarning: NAs introduced by coercion\n\ngdp$newyear &lt;- str_split_fixed(gdp$Year, \"\\\\[\", n = 2)[, 1]\ngdp$newyear &lt;- trimws(gdp$newyear)\n\ngdp &lt;- gdp[, c(\"Rank\", \"Country\", \"GDP_USD\", \"newyear\")]\n\nwrite.csv(gdp, \"gdp_table.csv\", row.names = FALSE)\n\nprint(gdp, width = Inf)\n\n# A tibble: 222 × 4\n    Rank Country     GDP_USD     newyear    \n   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt;      \n 1    NA 117,165,394 111,326,370 100,834,796\n 2    NA 30,615,743  29,184,890  27,720,700 \n 3    NA 19,398,577  18,743,803  17,794,782 \n 4    NA 5,013,574   4,659,929   4,525,704  \n 5    NA 4,279,828   4,026,211   4,204,495  \n 6    NA 4,125,213   3,912,686   3,575,778  \n 7    NA 3,958,780   3,643,834   3,380,855  \n 8    NA 3,361,557   3,162,079   3,051,832  \n 9    NA 2,543,677   2,372,775   2,300,941  \n10    NA 2,540,656   2,173,836   2,008,419  \n# ℹ 212 more rows"
  },
  {
    "objectID": "assignment03.html",
    "href": "assignment03.html",
    "title": "Assignment 3: Mapping Census Data",
    "section": "",
    "text": "Mapping Census Data\nIn this assignment, I decided to adjust the tidycensus01.R code to look at the labor force dynamics across Michigan counties from ACS. The analysis looks at the size of the civilian labor force and the number of the unemployed population. The analysis identifies the counties with the highest and lowest unemployment rates.\n\n# Packages\nlibrary(tidycensus)\nlibrary(tigris)\n\nTo enable caching of data, set `options(tigris_use_cache = TRUE)`\nin your R script or .Rprofile.\n\nlibrary(sf)\n\nLinking to GEOS 3.13.0, GDAL 3.8.5, PROJ 9.5.1; sf_use_s2() is TRUE\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(readr)\n\noptions(tigris_use_cache = TRUE)\n\n# 1) API key (uncomment and paste your key)\ncensus_api_key(\"991adc56800ae3c63abc4d1f74bc78711191233d\", install = FALSE)\n\nTo install your API key for use in future sessions, run this function with `install = TRUE`.\n\n# 2) Explore variables\nvars &lt;- load_variables(2023, \"acs5\", cache = TRUE)\n\n# 3) Parameters\nstate_abbr &lt;- \"MI\"\ngeo_level  &lt;- \"county\"   # options: state, county, tract, block group\nmy_vars    &lt;- c(labor_force = \"B23025_003\", unemployed = \"B23025_005\")\nyear_acs   &lt;- 2022\nsurvey     &lt;- \"acs5\"\n\n# 4) Download\nacs &lt;- get_acs(\n  geography = geo_level,\n  variables = my_vars,\n  state = state_abbr,\n  year = year_acs,\n  survey = survey,\n  geometry = TRUE\n)\n\nGetting data from the 2018-2022 5-year ACS\n\n# 5) Wide format for convenience\nacs_wide &lt;- acs |&gt;\n  tidyr::pivot_wider(\n    id_cols = c(GEOID, NAME, geometry),\n    names_from = variable,\n    values_from = c(estimate, moe)\n  ) |&gt;\n  # Add computed rate here\n  mutate(\n    unemployment_rate = estimate_unemployed / estimate_labor_force\n  )\n\n\n# 6) Map (Unemployment Rate)\nggplot(acs_wide) +\n  geom_sf(aes(fill = unemployment_rate), color = NA) +\n  scale_fill_viridis_c(name = \"Unemployment Rate\", labels = scales::percent) +\n  labs(\n    title = paste0(\"ACS \", year_acs, \" 5-year: Unemployment Rate — \", state_abbr, \" (\", geo_level, \")\"),\n    caption = \"Source: U.S. Census Bureau via tidycensus\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n# 7) Table (top/bottom by unemployment rate)\ntop10 &lt;- acs_wide |&gt;\n  arrange(desc(unemployment_rate)) |&gt;\n  select(NAME, unemployment_rate, estimate_unemployed, estimate_labor_force) |&gt;\n  slice_head(n = 10)\n\nbottom10 &lt;- acs_wide |&gt;\n  arrange(unemployment_rate) |&gt;\n  select(NAME, unemployment_rate, estimate_unemployed, estimate_labor_force) |&gt;\n  slice_head(n = 10)\n\n\ntop10\n\nSimple feature collection with 10 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -86.6156 ymin: 42.03334 xmax: -82.87024 ymax: 46.77332\nGeodetic CRS:  NAD83\n# A tibble: 10 × 5\n   NAME               unemployment_rate estimate_unemployed estimate_labor_force\n   &lt;chr&gt;                          &lt;dbl&gt;               &lt;dbl&gt;                &lt;dbl&gt;\n 1 Mackinac County, …            0.101                  490                 4839\n 2 Clare County, Mic…            0.0930                1132                12173\n 3 Wayne County, Mic…            0.0897               75008               835963\n 4 Oscoda County, Mi…            0.0889                 270                 3038\n 5 Genesee County, M…            0.0849               16087               189531\n 6 Luce County, Mich…            0.0830                 132                 1591\n 7 Schoolcraft Count…            0.0792                 274                 3460\n 8 Chippewa County, …            0.0770                1310                17021\n 9 Osceola County, M…            0.0766                 748                 9762\n10 Muskegon County, …            0.0746                6217                83299\n# ℹ 1 more variable: geometry &lt;MULTIPOLYGON [°]&gt;\n\nbottom10\n\nSimple feature collection with 10 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -88.13611 ymin: 42.41889 xmax: -83.66481 ymax: 46.24686\nGeodetic CRS:  NAD83\n# A tibble: 10 × 5\n   NAME               unemployment_rate estimate_unemployed estimate_labor_force\n   &lt;chr&gt;                          &lt;dbl&gt;               &lt;dbl&gt;                &lt;dbl&gt;\n 1 Leelanau County, …            0.0321                 322                10042\n 2 Allegan County, M…            0.0337                1991                59038\n 3 Clinton County, M…            0.0370                1511                40804\n 4 Ottawa County, Mi…            0.0378                6024               159492\n 5 Emmet County, Mic…            0.0379                 673                17748\n 6 Charlevoix County…            0.0381                 499                13114\n 7 Dickinson County,…            0.0382                 472                12348\n 8 Grand Traverse Co…            0.0390                1968                50437\n 9 Livingston County…            0.0396                4059               102593\n10 Shiawassee County…            0.0405                1349                33281\n# ℹ 1 more variable: geometry &lt;MULTIPOLYGON [°]&gt;\n\n# 8) Save outputs (optional)\nreadr::write_csv(st_drop_geometry(acs_wide), \"acs_data.csv\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Janette Phan",
    "section": "",
    "text": "I’m Janette Phan, a graduate student in Applied Cognition and Neuroscience at UT Dallas. My background is in Cognitive Science with a focus on cognitive neuroscience and AI, and I’m interested in using data and research to better understand how people think, learn, and interact with technology."
  },
  {
    "objectID": "assignment04.html",
    "href": "assignment04.html",
    "title": "Assignment 4",
    "section": "",
    "text": "# install.packages(\"tidyverse\")\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# install.packages(\"rvest\")\nlibrary(rvest)\n\n\nAttaching package: 'rvest'\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\n# install.packages(\"stringr\")\nlibrary(stringr)\n\nurl &lt;- 'https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)'\n# Reading the HTML code from the Wiki website\nwiki_gdp &lt;- read_html(url)\nclass(wiki_gdp)\n\n[1] \"xml_document\" \"xml_node\"    \n\ngdp_tables &lt;- wiki_gdp %&gt;%\n  html_nodes(xpath='//*[@id=\"mw-content-text\"]/div[1]/table[2]') %&gt;%\n  html_table()\nclass(gdp_tables)\n\n[1] \"list\"\n\ngdp &lt;- gdp_tables[[1]]\n\nnames(gdp) &lt;- c(\"Rank\", \"Country\", \"GDP_USD\", \"Year\", \"Source\")\n\nWarning: The `value` argument of `names&lt;-()` must have the same length as `x` as of\ntibble 3.0.0.\n\ncolnames(gdp)\n\n[1] \"Rank\"    \"Country\" \"GDP_USD\" \"Year\"   \n\nhead(gdp$Country, n = 10)\n\n [1] \"117,165,394\" \"30,615,743\"  \"19,398,577\"  \"5,013,574\"   \"4,279,828\"  \n [6] \"4,125,213\"   \"3,958,780\"   \"3,361,557\"   \"2,543,677\"   \"2,540,656\"  \n\n## Clean up variables\n## What type is Rank?\n## How about Date? How to fix it?\ngdp$Rank &lt;- as.numeric(gdp$Rank)\n\nWarning: NAs introduced by coercion\n\ngdp$newyear &lt;- str_split_fixed(gdp$Year, \"\\\\[\", n = 2)[, 1]\ngdp$newyear &lt;- trimws(gdp$newyear)\n\ngdp &lt;- gdp[, c(\"Rank\", \"Country\", \"GDP_USD\", \"newyear\")]\n\nwrite.csv(gdp, \"gdp_table.csv\", row.names = FALSE)\n\nprint(gdp, width = Inf)\n\n# A tibble: 222 × 4\n    Rank Country     GDP_USD     newyear    \n   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt;      \n 1    NA 117,165,394 111,326,370 100,834,796\n 2    NA 30,615,743  29,184,890  27,720,700 \n 3    NA 19,398,577  18,743,803  17,794,782 \n 4    NA 5,013,574   4,659,929   4,525,704  \n 5    NA 4,279,828   4,026,211   4,204,495  \n 6    NA 4,125,213   3,912,686   3,575,778  \n 7    NA 3,958,780   3,643,834   3,380,855  \n 8    NA 3,361,557   3,162,079   3,051,832  \n 9    NA 2,543,677   2,372,775   2,300,941  \n10    NA 2,540,656   2,173,836   2,008,419  \n# ℹ 212 more rows"
  },
  {
    "objectID": "assignment05.html",
    "href": "assignment05.html",
    "title": "assignment05",
    "section": "",
    "text": "## Scraping Government data\n## Website: GovInfo (https://www.govinfo.gov/app/search/)\n## Prerequisite: Download from website the list of files to be downloaded\n## Designed for background job\n\n# Start with a clean plate and lean loading to save memory\n \ngc(reset=T)\n\n          used (Mb) gc trigger (Mb) limit (Mb) max used (Mb)\nNcells  608482 32.5    1380704 73.8         NA   608482 32.5\nVcells 1117757  8.6    8388608 64.0      18432  1117757  8.6\n\n# install.packages(c(\"purrr\", \"magrittr\")\nlibrary(purrr)\nlibrary(magrittr) # Alternatively, load tidyverse\n\n\nAttaching package: 'magrittr'\n\n\nThe following object is masked from 'package:purrr':\n\n    set_names\n\n## Set path for reading the listing and home directory\n## For Windows, use \"c:\\\\directory\\\\subdirectory\\\\\"\n## For Mac, \"/Users/YOURNAME/path/\"\n\nsetwd(\"/Users/janettephan/documents/GitHub/phanjanette.github.io/assignment05\")\nlibrary(rjson)\nlibrary(jsonlite)\n\n\nAttaching package: 'jsonlite'\n\n\nThe following objects are masked from 'package:rjson':\n\n    fromJSON, toJSON\n\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\nlibrary(data.table)\n\n\nAttaching package: 'data.table'\n\n\nThe following object is masked from 'package:purrr':\n\n    transpose\n\nlibrary(readr)\n\n## CSV method\ngovfiles= read.csv(file=\"https://github.com/datageneration/datamethods/raw/refs/heads/master/webdata/govinfo-search-results-2024-10-13T07_10_42.csv\", skip=2)\n\n## JSON method\n### rjson\ngf_list &lt;- rjson::fromJSON(file =\"https://github.com/datageneration/datamethods/raw/refs/heads/master/webdata/govinfo-search-results-2024-10-13T07_18_29.json\")\ngovfile2=dplyr::bind_rows(gf_list$resultSet)\n\n### jsonlite\ngf_list1 = jsonlite::read_json(\"https://github.com/datageneration/datamethods/raw/refs/heads/master/webdata/govinfo-search-results-2024-10-13T07_18_29.json\")\n\n### Extract the list\ngovfiles3 &lt;- gf_list1$resultSet\n\n### One more step\ngovfiles3 &lt;- gf_list1$resultSet |&gt; dplyr::bind_rows()\n\n### Sorting publishDate newest to oldest\ngovfiles3 &lt;- govfiles3 |&gt;\n  dplyr::mutate(publishdate = as.Date(publishdate)) |&gt;\n  dplyr::arrange(dplyr::desc(publishdate))\n\n# Preparing for bulk download of government documents\ngovfiles$id = govfiles$packageId\npdf_govfiles_url = govfiles3$pdfLink\npdf_govfiles_id &lt;- govfiles3$index\n\n# Directory to save the pdf's\n# Be sure to create a folder for storing the pdf's\nsave_dir &lt;- \"/Users/janettephan/documents/GitHub/phanjanette.github.io/assignment05\"\n\n# Function to download pdfs\ndownload_govfiles_pdf &lt;- function(url, id) {\n  tryCatch({\n    destfile &lt;- paste0(save_dir, \"govfiles_\", id, \".pdf\")\n    download.file(url, destfile = destfile, mode = \"wb\") # Binary files\n    Sys.sleep(runif(1, 1, 3))  # Important: random sleep between 1 and 3 seconds to avoid suspicion of \"hacking\" the server\n    return(paste(\"Successfully downloaded:\", url))\n  },\n  error = function(e) {\n    return(paste(\"Failed to download:\", url))\n  })\n}\n\n### Download 10 most recent documents\nstart.time &lt;- Sys.time()\nmessage(\"Starting downloads\")\n\nStarting downloads\n\nresults &lt;- 1:10 %&gt;% \n  purrr::map_chr(~ download_govfiles_pdf(pdf_govfiles_url[.], pdf_govfiles_id[.]))\nmessage(\"Finished downloads\")\n\nFinished downloads\n\nend.time &lt;- Sys.time()\ntime.taken &lt;- end.time - start.time\ntime.taken\n\nTime difference of 25.08157 secs\n\nprint(results)\n\n [1] \"Successfully downloaded: https://www.govinfo.gov/content/pkg/BILLS-118sres890is/pdf/BILLS-118sres890is.pdf\"        \n [2] \"Successfully downloaded: https://www.govinfo.gov/content/pkg/BILLS-118sjres114is/pdf/BILLS-118sjres114is.pdf\"      \n [3] \"Successfully downloaded: https://www.govinfo.gov/content/pkg/BILLS-118sres805ats/pdf/BILLS-118sres805ats.pdf\"      \n [4] \"Successfully downloaded: https://www.govinfo.gov/content/pkg/BILLS-118sjres115is/pdf/BILLS-118sjres115is.pdf\"      \n [5] \"Successfully downloaded: https://www.govinfo.gov/content/pkg/BILLS-118sjres113is/pdf/BILLS-118sjres113is.pdf\"      \n [6] \"Successfully downloaded: https://www.govinfo.gov/content/pkg/BILLS-118sjres111is/pdf/BILLS-118sjres111is.pdf\"      \n [7] \"Successfully downloaded: https://www.govinfo.gov/content/pkg/BILLS-118sjres112is/pdf/BILLS-118sjres112is.pdf\"      \n [8] \"Successfully downloaded: https://www.govinfo.gov/content/pkg/CREC-2024-09-25/pdf/CREC-2024-09-25-pt1-PgD951.pdf\"   \n [9] \"Successfully downloaded: https://www.govinfo.gov/content/pkg/CREC-2024-09-25/pdf/CREC-2024-09-25-pt1-PgS6417-2.pdf\"\n[10] \"Successfully downloaded: https://www.govinfo.gov/content/pkg/CREC-2024-09-25/pdf/CREC-2024-09-25-pt1-PgS6418.pdf\""
  },
  {
    "objectID": "assignment05.html#difficulties-encountered",
    "href": "assignment05.html#difficulties-encountered",
    "title": "assignment05",
    "section": "Difficulties Encountered:",
    "text": "Difficulties Encountered:\nI had trouble figuring out the correct date variable because it wasn’t clearly documented in the dataset. Setting up the working directory also took time since I needed to find the right folder where my downloaded files would go and be easy to locate. Another challenge was simply understanding the code itself, as it took a while to learn what each part was doing and how the pieces worked together."
  },
  {
    "objectID": "assignment05.html#usefulness-of-the-scraped-data",
    "href": "assignment05.html#usefulness-of-the-scraped-data",
    "title": "assignment05",
    "section": "Usefulness of the Scraped Data:",
    "text": "Usefulness of the Scraped Data:\nThe scraped data is usable and includes the main information I expected, such as the document title, text, and publication date. After skimming the downloaded documents, they appear accurate, though they would still need closer review to confirm nothing important is missing."
  },
  {
    "objectID": "assignment05.html#how-to-improve",
    "href": "assignment05.html#how-to-improve",
    "title": "assignment05",
    "section": "How to Improve?:",
    "text": "How to Improve?:\nThe scraping process could be improved by using an API instead of downloading files manually. An API would provide cleaner data and make the process faster and more reliable."
  }
]